---
title: "The Contrast Method for Literature-Based Discovery - Part 2: Short Reproducibility Study"
author: "Erwan Moreau"
date: "May 2021"
output: html_document
---

```{r setup, include=FALSE}
library(knitr)
library(rmarkdown)
knitr::opts_chunk$set(echo = TRUE)
```

# Overview

This is the documentation of the companion code for the paper **TODO**, which presents a novel approach for Literature-Based Discovery. The approach relies on contrasting different "views" of the data, the data being the biomedical literature (Medline and PubMed Central). The experiments use 8 relations which correspond to past discoveries in the domain of Neurodegenerative Diseases (NDs).

- The paper can be found at **TODO**
- The code is available at https://github.com/erwanm/lbd-contrast

For running these experiments two options are proposed: 

- The "full replication" offers more flexibility but requires much more computing resources (access to a cluster is recommended). For this option please follow the instructions in [Part 1](./part1-full-replication.html).
- The "short reproducibility study" proposes to use the exact same preprocessed dataset as described in the paper which can be downloaded from **TODO**. For this option please follow the instructions in this document.


## Data Requirements for the Short Reproducibility Study

For convenience the fully preprocessed dataset used in the paper is provided here **TODO**. The user should be able to reproduce the results presented in the paper by following the instructions below.


## Software Requirements

- This code requires R and a few R libraries: 
    - `reshape2`
    - `ggplot2`
    - `plyr`
    
The code can be loaded in the R interpreter with:

```{r source}
source('contrast-experiments.R')
```

## Hardware Requirements

16 GB RAM recommended.


## Preparing for The Contrasts Experiments

### Loading the Datasets



```{r load}
lbd_data<-read.table('lbd_data.tsv',header=TRUE,sep='\t',comment.char = "",quote="")
lbd_data_diso<-read.table('lbd_data.DISO.tsv',header=TRUE,sep='\t',comment.char = "",quote="")
targetGoldPairs <- read.table('targets-gold-pairs-with-year-range.tsv',header=TRUE,sep='\t',comment.char = "",quote="")
```


```{r}
colnames(lbd_data)
```

For every target concept (column `targetId`), the data contains:

- Two dataset variants (`dataset`) with their corresponding `targetId`
- Several ranges of year (columns `start`, `end`)
- Several views of the co-occurring concepts (columns `view`, `level`)


### Calculating Association Measures

```{r}
lbd_data <- calculateAssociation(lbd_data)
lbd_data_diso <- calculateAssociation(lbd_data_diso)
```

This adds a column for every measure: `pmi`, `npmi`, `mi`, `pmi2`, `pmi3`.

### Specifying Methods Parameters

The parameters of the various methods are provided as a dataframe where each row contains the method id and the parameters. A few standard configuration methods can be generated with the `generateMethods()` function. For instance the following generates 15 methods using only PMI as association measure:

```{r}
methods0 <- generateMethods(measures='pmi')
```

The following will generate the same set of methods with the minimum frequency parameter set to either 1 or 3 (30 methods):

```{r}
methods1 <- generateMethods(measures='pmi',minFreqThresholds = c(1,3))
```

## Applying Methods

In this part we explain how to use a few important functions. The examples are provided as illustrations, they are not necessary for reproducing the experiments.

### Basic Application

`contrastViews` is the core function applying the contrast method. 

In the following we select:

- the target ALS with the range of years 1950-2003
- a reference view "abstracts+articles" by document
- a mask view "unfiltered-medline" by sentence

```{r}
als <- with(lbd_data_diso,lbd_data_diso[dataset=='KD' &targetName=='ALS' & end==2003,])
als_ref <- with(als, als[view=='abstracts+articles'& level=='by-doc',])
als_mask <- with(als, als[view=='unfiltered-medline'& level=='by-sent',])
```

The basic contrast method expects the rank and/or relative rank as columns. Here the ranking is based on the NPMI measure. 

- *Important*: a low rank corresponds to a strong association, i.e. the top rank 1 is the strongest associated concept. This is why the ranking is made according to the opposite of the NPMI value. 

```{r}
   als_ref$rank <- rank(-als_ref[,'npmi'])
   als_ref$relRank <- als_ref$rank / nrow(als_ref)
   als_mask$rank <- rank(-als_mask[,'npmi'])
   als_mask$relRank <- als_mask$rank / nrow(als_mask)
```

```{r}
r <- contrastViews(als_ref,als_mask)
kable(head(r))
```

### Single Target with Multiple Methods

`applyMethodsSingleTarget` applies multiple methods to a single target concept and range of years. The input data must contain every possible view for the target so that any method can be applied. The function can apply either a baseline or contrast method.

It returns a tidy dataframe containing for every method:

- All the method parameters
- The full ranking obtained from applying the method to the input target.

```{r}
r<-applyMethodsSingleTarget(als,methods0)
kable(r[sample(1:nrow(r),6),])
```

### Multiple Targets with Multiple Methods

`applyMethodsMultiTargets` applies multiple methods to multiple target-gold pairs and returns only the results for each specfic pair.

- As opposed to `applyMethodsSingleTarget`, the input contains a "gold concept" for every target concept. The "gold concept" is a concept of interest expected to (ideally) reach a top position in the ranking.
- For every method and every target-gold pair, the output contains only the row corresponding to the gold in the output ranking.


```{r}
r <- applyMethodsMultiTargets(lbd_data_diso,targetGoldPairs,methods0)
print(colnames(r))
kable(r[sample(1:nrow(r),6),])
```

### Evaluation

`evalByGroup` returns performance scores for every group of target-gold pairs in a dataframe obtained from `applyMethodsMultiTargets`. By default the groups are made of rows sharing the exact same method parameters, but this can be modified with the parameter `groupBy`. The
optional parameter `nbTargetGoldPairs` can be used to check that the number of pairs is the same as expected (sanity check, a warning is raised if not satisfied) but also to force the evaluation measures to calculate performance based on this number rather than the actual number of rows.

```{r}
perf <- evalByGroup(r, nbTargetGoldPairs=8)
kable(perf)
```

### Leave-One-Out Cross-Validation

`crossValidateMethodsLOO` performs parameter tuning with Leave-One-Out (LOO) cross-validation over a set of target-gold pairs for several groups of methods. Some of the methods parameters can be fixed with `resultsGroupBy`, while any other variations in the parameters provided in `methodsDF` are used for tuning. Optionally the min/max thresholds parameters can be tuned as well by setting `optimMinMaxThresholds=TRUE`. 

```{r}
bestByMethodId <- crossValidateMethodsLOO(lbd_data_diso,targetGoldPairs,methods1,optimMinMaxThresholds=FALSE,returnDetailByTargetGoldPairs=TRUE)
```

By default a dataframe is returned which contains the performance on the test set for every group of fixed parameters. If `returnDetailByTargetGoldPairs` is `TRUE`, a list is returned which contains two dataframes: 

- `aggregatedPerf` is the regular performance dataframe on the test set.
- `detailedResults` is the dataframe containing results for every target-gold pair during training. This can be used to analyze how often every method parameter is selected as the best in its group.

```{r}
kable(bestByMethodId$aggregatedPerf)
```

## Experiments


### Experiment 1: Evaluation of a few Simple Methods


```{r simple}
methods0 <- generateMethods(measures=c('scp','pmi','npmi','mi'),minFreqThresholds=c(1,3,5),contrastMethodsIds = c('diffAbsRank', 'basicContrast'))
methods0 <- methods0[methods0$refView!='pmc-articles' & (methods0$refView!='unfiltered-medline' | methods0$refLevel=='by-doc'),]
res0 <- applyMethodsMultiTargets(lbd_data,targetGoldPairs,methods0)
perf0 <- evalByGroup(res0, nbTargetGoldPairs=8)
write.table(res0, 'simple.tsv',sep='\t',quote = FALSE,row.names=FALSE)
kable(perf0)
```

### Experiment 2: Comparing Methods Parameters

```{r compare1}
methods1 <- generateMethods(measures=c('pmi','npmi'),minFreqThresholds = 1:20)
res1 <- applyMethodsMultiTargets(lbd_data,targetGoldPairs,methods1)
perf1 <- evalByGroup(res1, nbTargetGoldPairs=8)
write.table(res1, 'param-minFreq.tsv',sep='\t',quote = FALSE,row.names=FALSE)
```

```{r compare2}
methods2 <- generateMethods(measures=c('pmi','npmi','mi','pmi2','pmi3'),minFreqThresholds = c(1,3,5))
res2 <- applyMethodsMultiTargets(lbd_data,targetGoldPairs,methods2)
perf2 <- evalByGroup(res2, nbTargetGoldPairs=8)
write.table(res2, 'param-measures.tsv',sep='\t',quote = FALSE,row.names=FALSE)
```


```{r compare3}
methods3 <- generateMethods(measures=c('pmi','npmi'),minFreqThresholds = c(1,3,5),refMaskPairs=NULL)
res3 <- applyMethodsMultiTargets(lbd_data,targetGoldPairs,methods3)
perf3 <- evalByGroup(res3, nbTargetGoldPairs=8)
write.table(res3, 'param-views.tsv',sep='\t',quote = FALSE,row.names=FALSE)
```

```{r compare4}
methods4 <- generateMethods(measures=c('pmi','npmi'),minFreqThresholds = c(1,3,5),contrastDiscardRowsNotInMaskView=c(TRUE,FALSE))
res4 <- applyMethodsMultiTargets(lbd_data,targetGoldPairs,methods4)
perf4 <- evalByGroup(res4, nbTargetGoldPairs=8)
write.table(res4, 'param-discardRows.tsv',sep='\t',quote = FALSE,row.names=FALSE)
```

### Experiment 3: Parameter Tuning with Leave-One-Out Cross-Validation

```{r loo1}
all_methods <- generateMethods(refMaskPairs=NULL,contrastDiscardRowsNotInMaskView=c(TRUE,FALSE))
system.time(bestByMethodIdNoOptim <- crossValidateMethodsLOO(lbd_data,targetGoldPairs,all_methods,optimMinMaxThresholds=FALSE,returnDetailByTargetGoldPairs=TRUE))
write.table(bestByMethodIdNoOptim$detailedResults,'loo-no-optim.tsv',sep='\t',quote = FALSE,row.names=FALSE)
```


```{r loo2}
system.time(bestByMethodIdWithOptim <- crossValidateMethodsLOO(lbd_data,targetGoldPairs,all_methods,optimMinMaxThresholds=TRUE,returnDetailByTargetGoldPairs=TRUE))
write.table(bestByMethodIdWithOptim$detailedResults,'loo-with-optim.tsv',sep='\t',quote = FALSE,row.names=FALSE)
```

