---
title: "The Contrast Method for Literature-Based Discovery - Part 2: Short Reproducibility Study"
author: "Erwan Moreau"
date: "May 2021"
output: html_document
---

```{r setup, include=FALSE}
library(knitr)
library(rmarkdown)
knitr::opts_chunk$set(echo = TRUE)
```

# Overview

This is the documentation of the companion code for the paper **TODO**, which presents a novel approach for Literature-Based Discovery. The approach relies on contrasting different "views" of the data, the data being the biomedical literature (Medline and PubMed Central). The experiments use 8 relations which correspond to past discoveries in the domain of Neurodegenerative Diseases (NDs).

- The paper can be found at **TODO**
- The code is available at https://github.com/erwanm/lbd-contrast

For running these experiments two options are proposed: 

- The "full replication" offers more flexibility but requires much more computing resources (access to a cluster is recommended). For this option please follow the instructions in [Part 1](./part1-full-replication.html).
- The "short reproducibility study" proposes to use the exact same preprocessed dataset as described in the paper which can be downloaded from **TODO**. For this option please follow the instructions in this document.


## Data Requirements for the Short Reproducibility Study

For convenience the fully preprocessed dataset used in the paper is provided here **TODO**. The user should be able to reproduce the results presented in the paper by following the instructions below.


## Software Requirements

- This code requires R and a few R libraries: 
    - `reshape2`
    - `ggplot2`
    - `plyr`
    
The code can be loaded in the R interpreter with:

```{r}
source('contrast-experiments.R')
```

## Hardware Requirements

A recent enough computer with at least 8 GB RAM (16 GB recommended).


## Preparing for The Contrasts Experiments

### Loading the Datasets



```{r}
lbd_data<-read.table('lbd_data.tsv',header=TRUE,sep='\t',comment.char = "",quote="")
lbd_data_diso<-read.table('lbd_data.DISO.tsv',header=TRUE,sep='\t',comment.char = "",quote="")
```

```{r}
targetGoldPairs <- read.table('targets-gold-pairs-with-year-range.tsv',header=TRUE,sep='\t',comment.char = "",quote="")
```


```{r}
colnames(lbd_data)
```

For every target concept (column `targetId`), the data contains:

- Two dataset variants (`dataset`) with their corresponding `targetId`
- Several ranges of year (columns `start`, `end`)
- Several views of the co-occurring concepts (columns `view`, `level`)


### Calculating Association Measures

```{r}
lbd_data <- calculateAssociation(lbd_data)
lbd_data_diso <- calculateAssociation(lbd_data_diso)
```

This adds a column for every measure: `pmi`, `npmi`, `mi`, `pmi2`, `pmi3`.

### Specifying Methods Parameters

The parameters of the various methods are provided as a dataframe where each row contains the method id and the parameters. A few standard configuration methods can be generated with the `generateMethods()` function. For instance the following generates 15 methods using only PMI as association measure:

```{r}
methods0 <- generateMethods(measures='pmi')
```

The following will generate the same set of methods with the minimum frequency parameter set to either 1 or 3 (30 methods):

```{r}
methods1 <- generateMethods(measures='pmi',minFreqThresholds = c(1,3))
```

## Applying Methods

In this part we explain how to use a few important functions. The examples are provided as illustrations, they are not necessary for reproducing the experiments.

### Basic Application

`contrastViews` is the core function applying the contrast method. 

In the following we select:

- the target ALS with the range of years 1950-2003
- a reference view "abstracts+articles" by document
- a mask view "unfiltered-medline" by sentence

```{r}
als <- with(lbd_data_diso,lbd_data_diso[dataset=='KD' &targetName=='ALS' & end==2003,])
als_ref <- with(als, als[view=='abstracts+articles'& level=='by-doc',])
als_mask <- with(als, als[view=='unfiltered-medline'& level=='by-sent',])
```

The basic contrast method expects the rank and/or relative rank as columns. Here the ranking is based on the NPMI measure. 

- *Important*: a low rank corresponds to a strong association, i.e. the top rank 1 is the strongest associated concept. This is why the ranking is made according to the opposite of the NPMI value. 

```{r}
   als_ref$rank <- rank(-als_ref[,'npmi'])
   als_ref$relRank <- als_ref$rank / nrow(als_ref)
   als_mask$rank <- rank(-als_mask[,'npmi'])
   als_mask$relRank <- als_mask$rank / nrow(als_mask)
```

```{r}
r <- contrastViews(als_ref,als_mask)
kable(head(r))
```

### Single Target with Multiple Methods

`applyMethodsSingleTarget` applies multiple methods to a single target concept and range of years. The input data must contain every possible view for the target so that any method can be applied. The function can apply either a baseline or contrast method.

It returns a tidy dataframe containing for every method:

- All the method parameters
- The full ranking obtained from applying the method to the input target.

```{r}
r<-applyMethodsSingleTarget(als,methods0)
kable(r[sample(1:nrow(r),6),])
```

### Multiple Targets with Multiple Methods

`applyMethodsMultiTargets` applies multiple methods to multiple target-gold pairs and returns only the results for each specfic pair.

- As opposed to `applyMethodsSingleTarget`, the input contains a "gold concept" for every target concept. The "gold concept" is a concept of interest expected to (ideally) reach a top position in the ranking.
- For every method and every target-gold pair, the output contains only the row corresponding to the gold in the output ranking.


```{r}
r <- applyMethodsMultiTargets(lbd_data_diso,targetGoldPairs,methods0)
print(colnames(r))
kable(r[sample(1:nrow(r),6),])
```

### Evaluation

`evalByGroup` returns performance scores for every group of target-gold pairs in a dataframe obtained from `applyMethodsMultiTargets`. By default the groups are made of rows sharing the exact same method parameters, but this can be modified with the parameter `groupBy`. The
optional parameter `nbTargetGoldPairs` can be used to check that the number of pairs is the same as expected (sanity check, a warning is raised if not satisfied) but also to force the evaluation measures to calculate performance based on this number rather than the actual number of rows.

```{r}
perf <- evalByGroup(r, nbTargetGoldPairs=8)
kable(perf)
```

### Leave-One-Out Cross-Validation

