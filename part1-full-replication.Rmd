---
title: "The Contrast Method for Literature-Based Discovery - Part 1: Full Replication"
author: "Erwan Moreau"
date: "May 2021"
output: html_document
---

```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(echo = TRUE)
```

# Overview

This is the documentation of the companion code for the paper **TODO**, which presents a novel approach for Literature-Based Discovery. The approach relies on contrasting different "views" of the data, the data being the biomedical literature (Medline and PubMed Central). The experiments use 8 relations which correspond to past discoveries in the domain of Neurodegenerative Diseases (NDs).

- The paper can be found at **TODO**
- The code is available at https://github.com/erwanm/lbd-contrast

For running these experiments two options are proposed: 

- The "full replication" offers more flexibility but requires much more computing resources (access to a cluster is recommended). For this option:
    - First follow the instructions in this guide,
    - Then go to [Part 2](./part2-short-reproducibility-study.html).
- The "short reproducibility study" proposes to use the exact same preprocessed dataset as described in the paper which can be downloaded [here](https://zenodo.org/record/4895190). For this option you can go directly to [Part 2](./part2-short-reproducibility-study.html).


## Data Requirements for Full Replication

This option lets the user apply the contrast method to different source data and/or different target relations. However this requires (much) more effort and computational resources.

This study uses two sources for the biomedical literature. The software needed to preprocess these sources is presented below, please follow the corresponding instructions in the links. It is not strictly necessary to use the two sources for running the contrast method, one is enough.

- [PubTatorCentral](https://www.ncbi.nlm.nih.gov/research/pubtator/) conveniently provides Medline/PMC data after annotation and disambiguation. This data still needs to be converted to TDC format and pre-processed using The [TDC Tools code](https://github.com/erwanm/tdc-tools).
- The raw Medline/PMC data can be converted to TDC and disambiguated using the following tools:
    - [This fork of the Knowedge Discovery (KD) repository](https://github.com/erwanm/knowledgediscovery) (The [original code](https://github.com/jakelever/knowledgediscovery) was made by Jake Lever)
    - The in-house disambiguation code ["KD Data Tools"](https://github.com/erwanm/kd-data-tools)
    - The [TDC Tools code](https://github.com/erwanm/tdc-tools).

Once the raw data is ready, please follow the instructions below.

## Software Requirements

- This code requires R and a few R libraries: 
    - `reshape2`
    - `ggplot2`
    - `plyr`
    
The code can be loaded in the R interpreter with:

```{r}
source('contrast-experiments.R')
```

## Hardware Requirements

The full replication requires massive resources for computing the source data (see documentation of the required repositories), and at least 32 GB RAM for processing the data from scratch with the R code below. 


# Preparing the dataset

- Note 1: some of the commands below are run with `system.time()` in order to show how long they take to execute. This is optional of course.
- Note 2: in the following we present the simplest way to reproduce the experiments. Most of the functions accept optional arguments which may be useful for alternative tasks or data.
 
## Raw frequency data

The input data is made of at least one of the two sources:

- PubTator Central (PTC), obtained with [TDC Tools code](https://github.com/erwanm/tdc-tools). 
- Knowedge Discovery (KD), obtained with [my fork of the Knowedge Discovery (KD) repository](https://github.com/erwanm/knowledgediscovery), [KD Data Tools](https://github.com/erwanm/kd-data-tools) and [TDC Tools code](https://github.com/erwanm/tdc-tools).

The R code expects the data directory structure as: `<data dir>/<dataset>/<indiv|joint>/<level>/<view>/<year>[.total]` where:

- `<dataset>` is the id of the data, e.g. `PTC` or `KD`.
- subdirectories `indiv` and `joint` contain respectively the individual (resp. joint) frequencies of the concepts (resp. concepts relations) depending on the view and level, as computed by [TDC Tools code](https://github.com/erwanm/tdc-tools)).

```{r}
system.time(raw<-loadData(verbose=FALSE))
```

- Note: the PTC concepts are expected to be read as `<id>@<type>`; the `@<type>` suffix is removed by default.

`raw` is a list made of three dataframes `indiv`, `joint` and `total`:

```{r}
colnames(raw$indiv)
colnames(raw$joint)
colnames(raw$total)
```

## Target Concepts and Discoveries

### Target Concepts

By default individual target concepts are provided in the file `data/targets.list` which can be loaded as:

```{r}
targets <- loadTargetsList()
kable(targets)
```

The content should contain for every target the corresponding Mesh descriptor and UMLS concept id (CUI).



### Target Discoveries

By default the pairs of concepts representing the target discoveries are provided in the file `data/discoveries.tsv` which can be loaded as:

```{r}
discoveries <- loadDiscoveries()
kable(discoveries)
```

### Preparing Target Pairs

The individual target ids and discovery relations are combined in a dataframe which provides the relevant id (Mesh or UMLS) depending on the source dataset:

```{r}
targetPairs <- preprocessDiscoveries(discoveries,targets)
kable(targetPairs)
```


## Determine a range of years for every discovery

- Note: the steps below are needed only for the purpose of evaluating the method using past discoveries as gold standard. In an exploratory setting one would simply aggregate all the years for which data is available.

### Filtering the raw data

```{r}
targetPairsJointData <- filterJointPairs(targetPairs,raw$joint)
kable(targetPairsJointData[sample(1:nrow(targetPairsJointData), 6),])
```

### Joint frequency across years

The following shows the earliest year where a co-occurrence was found for every relation:

```{r}
firstYear <- ddply(targetPairsJointData,c('concept1','concept2'), function(s) { data.frame(firstYear=min(s$year))})
kable(firstYear)
```




```{r, out.width="100%"}
plotTargetDiscoveriesJointFreq(targetPairsJointData,customTheme = TRUE,renameViews = TRUE)
```

### Calculating the Range of Years Before Discovery

The function below calculates the first year of the consecutive sequence where the two concepts have at least one occurrence every year (assumed to represent the "discovery"). It returns for every relation the range of years *before* this "discovery year": by default the start year is arbitrarily set to 1950, and the end year is the year just before the "discovery year".

```{r}
targetRanges <- selectYearRange(targetPairsJointData)
kable(targetRanges)
```


### Obtaining the Target-Gold Pairs

The function below re-organizes the above dataframe to provide a directed target-gold pair for every row.

- Set the second argument `bothDirections` as `FALSE` and leave `selectTargetNames` as `NULL` if the discovery pairs are already ordered target first.
- If not (as in the provided ND discoveries), use the following arguments where `selectTargetNames` indicates the two target ND diseases:

```{r}
targetGoldPairs <- convertTargetPairsToTargetGoldPairs(targetRanges, TRUE, c('ALS','FTD'))
```

Optionally the target-gold pairs dataframe can be saved for futher usage:

```{r}
write.table(targetGoldPairs, 'targets-gold-pairs-with-year-range.tsv',sep='\t',quote = FALSE,row.names=FALSE)
```

### Aggregating frequencies across years

This step sums the individual, joint and total frequencies in the `raw` list across the years specified by `targetRanges` for every required range of years. 

```{r}
system.time(aggregated<-aggregateFullDataAcrossYears(raw, targetRanges))
```

`aggregated` is a list made of 3 dataframes `indiv`, `joint` and `total`.


## Preparing the final dataset

The following step integrates the 3 dataframes together so that each row contains all the relevant frequencies for the corresponding pair of concepts.

```{r}
system.time(integrated<-integratePairsDataByYearRange(aggregated))
kable(integrated[sample(1:nrow(integrated),6),])
```


Finally the dataframe is reformated for convenient access by pair (target concept, related concept): for every target concept in `targetPairs` and any relation (A,B) in `aggregated`, the target (A or B) is placed as the first concept. If both A and B are targets, rows are added to represent the two cases. This way from the output dataframe a full list of relations can be selected for any specific target.


```{r}
lbd_data<-reformatRelationsByTarget(integrated, targetPairs)
kable(lbd_data[sample(1:nrow(lbd_data),6),])
```

## Saving the final dataset

```{r}
write.table(lbd_data, 'lbd_data.tsv',sep='\t',quote = FALSE,row.names=FALSE)
```

# Filtering Semantic Types (optional)

The filtering by semantic type is done outside R using UMLS data. Requirements:

- Download the [UMLS metathesaurus data](https://www.nlm.nih.gov/research/umls/index.html) 
- Download the [UMLS "Semantic Groups" file]( https://lhncbc.nlm.nih.gov/semanticnetwork/download/SemGroups.txt)

Several python scripts are provided in the `filter-sem-type` directory. In the following commands it is assumed that the [final dataset has been saved](#saving-the-final-dataset) as `lbd_data.tsv`. Below the "DISO" (disorder) concepts are selected, the same process can be used with any subset of semantic groups.

## Obtain Concepts ids for A Specific Group

Obtain concept ids as UMLS CUIs:

```
cd filter-sem-type
python3 filter-umls-semantic-types.py SemGroups.txt <UMLS dir> DISO >DISO.cui
```

Convert CUIs to MeSH descriptors:

```
cat DISO.cui | python3 umls-to-mesh.py <UMLS dir> >DISO0.mesh
cat DISO0.mesh | sed 's/^/MESH:/' >DISO.mesh
```
## Applying to LBD dataset

Separate the KD and PTC data (because KD uses UMLS ids whereas PTC uses MeSH ids):

```
echo KD | python3 filter-column.py lbd_data.tsv 5 >kd.tsv
echo PTC | python3 filter-column.py lbd_data.tsv 5 >ptc.tsv
```
Filter the two subsets separately:

```
cat DISO.cui | python3 filter-column.py kd.tsv 8 >kd.diso.tsv
cat DISO.mesh | python3 filter-column.py ptc.tsv 8 >ptc.diso.tsv
```

```
head -n 1 ../lbd_data.tsv >lbd_data.DISO.tsv
cat kd.diso.tsv ptc.diso.tsv >>lbd_data.DISO.tsv
```

